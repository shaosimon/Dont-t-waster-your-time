1、sublime 添加同时打开多个项目：project -> add Folder to project。

RDD是Spark数据结构最基本的抽象化概念之一，又名抽象弹性分布式数据库。
RDD是一个不可改写的数据体，immutable，你只可以从一个RDD经过变换生产出一个新的RDD，你不能直接修改RDD里的数据。
如果你熟悉functional programming应该可以理解这个概念。
一个RDD有3个必须的组成部分1，每个RDD都有很多块构成，这个抽象化概念是为了允许RDD被分布式处理。
用白话解释一下就是你有个1000行数据的RDD，那这个RDD会由100块10行的部分组成，也可以由一块1000行的部分组成。
关键是这些小块数据是RDD的组成部分。
2，我开头说的，你只可以从一个RDD变换生产出另一个RDD
，所以每个RDD都会记录他是从哪个RDD生出来的。RDD之间就产生了一个树状的关系。这个抽象化概念是为了数据恢复。
假设你丢失了一块RDD数据，由于有了这个概念，你可以找到丢掉的数据他妈，重新计算下你丢掉的那块数据。
3，第二点是说每个RDD都有个妈妈，第三点就是RDD是怎么生出来的。老王对RDD妈妈做了什么才造出了RDD儿子。
有这个概念也是为了数据恢复。打个比方，RDD_son是老妈RDD_mom做了一个map(x => x*2)把老妈的每个成员翻一倍产生的。
那这个.map(x => x*2)就是每个RDD的第三个组成部分。
所以经过这三点你可以看出RDD这个抽象化概念完全是为了网络分布式运算产生了。
首先他可以分成很多块，找很多机器一人计算一块。
其次，假设有台机器被偷了，有些数据丢了，你可以根据RDD的从属关系重新计算丢掉的数据。
RDD本身是一个interface，你只要能做到以上三点就可以写自己的RDD拿到Spark上运行。
知乎link:https://www.zhihu.com/question/59810584
3、checkout 检出，切换分支。
4、git command -help,查看名命令参数

